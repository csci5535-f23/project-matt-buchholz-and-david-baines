@inproceedings{Steinbakken,
    title = "Native-Language Identification with Attention",
    author = {Steinbakken, Stian  and
      Gamb{\"a}ck, Bj{\"o}rn},
    booktitle = "Proceedings of the 17th International Conference on Natural Language Processing (ICON)",
    month = dec,
    year = "2020",
    address = "Indian Institute of Technology Patna, Patna, India",
    publisher = "NLP Association of India (NLPAI)",
    url = "https://aclanthology.org/2020.icon-main.35",
    pages = "261--271",
    abstract = "The paper explores how an attention-based approach can increase performance on the task of native-language identification (NLI), i.e., to identify an author{'}s first language given information expressed in a second language. Previously, Support Vector Machines have consistently outperformed deep learning-based methods on the TOEFL11 data set, the de facto standard for evaluating NLI systems. The attention-based system BERT (Bidirectional Encoder Representations from Transformers) was first tested in isolation on the TOEFL11 data set, then used in a meta-classifier stack in combination with traditional techniques to produce an accuracy of 0.853. However, more labelled NLI data is now available, so BERT was also trained on the much larger Reddit-L2 data set, containing 50 times as many examples as previously used for English NLI, giving an accuracy of 0.902 on the Reddit-L2 in-domain test scenario, improving the state-of-the-art by 21.2 percentage points.",
}

@article{Tziafas,
    author       = {Giorgos Tziafas and
                  Konstantinos Kogkalidis and
                  Gijs Wijnholds and
                  Michael Moortgat},
    title        = {Improving {BERT} Pretraining with Syntactic Supervision},
    journal      = {CoRR},
    volume       = {abs/2104.10516},
    year         = {2021},
    url          = {https://arxiv.org/abs/2104.10516},
    eprinttype    = {arXiv},
    eprint       = {2104.10516},
    timestamp    = {Mon, 26 Apr 2021 17:25:10 +0200},
    biburl       = {https://dblp.org/rec/journals/corr/abs-2104-10516.bib},
    bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Koppel,
    author = {Koppel, Moshe and Schler, Jonathan and Zigdon, Kfir},
    year = {2005},
    month = {08},
    pages = {624-628},
    title = {Determining an author's native language by mining a text for errors},
    booktitle = {Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining},
    doi = {10.1145/1081870.1081947}
}

@article{translationese,
    author = {Volansky, Vered and Ordan, Noam and Wintner, Shuly},
    title = "{On the features of translationese}",
    journal = {Digital Scholarship in the Humanities},
    volume = {30},
    number = {1},
    pages = {98-118},
    year = {2013},
    month = {07},
    abstract = "{Much research in translation studies indicates that translated texts are ontologically different from original non-translated ones. Translated texts, in any language, can be considered a dialect of that language, known as ‘translationese’. Several characteristics of translationese have been proposed as universal in a series of hypotheses. In this work, we test these hypotheses using a computational methodology that is based on supervised machine learning. We define several classifiers that implement various linguistically informed features, and assess the degree to which different sets of features can distinguish between translated and original texts. We demonstrate that some feature sets are indeed good indicators of translationese, thereby corroborating some hypotheses, whereas others perform much worse (sometimes at chance level), indicating that some ‘universal’ assumptions have to be reconsidered.In memoriam: Miriam Shlesinger, 1947–2012}",
    issn = {2055-7671},
    doi = {10.1093/llc/fqt031},
    url = {https://doi.org/10.1093/llc/fqt031},
    eprint = {https://academic.oup.com/dsh/article-pdf/30/1/98/21521905/fqt031.pdf},
}




@article{Sarwar,
    author = {Sarwar, Raheem and Rutherford, Attapol T. and Hassan, Saeed-Ul and Rakthanmanon, Thanawin and Nutanong, Sarana},
    title = {Native Language Identification of Fluent and Advanced Non-Native Writers},
    year = {2020},
    issue_date = {July 2020},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {19},
    number = {4},
    issn = {2375-4699},
    url = {https://doi.org/10.1145/3383202},
    doi = {10.1145/3383202},
    abstract = {Native Language Identification (NLI) aims at identifying the native languages of authors by analyzing their text samples written in a non-native language. Most existing studies investigate this task for educational applications such as second language acquisition and require the learner corpora. This article performs NLI in a challenging context of the user-generated-content (UGC) where authors are fluent and advanced non-native speakers of a second language. Existing NLI studies with UGC (i) rely on the content-specific/social-network features and may not be generalizable to other domains and datasets, (ii) are unable to capture the variations of the language-usage-patterns within a text sample, and (iii) are not associated with any outlier handling mechanism. Moreover, since there is a sizable number of people who have acquired non-English second languages due to the economic and immigration policies, there is a need to gauge the applicability of NLI with UGC to other languages. Unlike existing solutions, we define a topic-independent feature space, which makes our solution generalizable to other domains and datasets. Based on our feature space, we present a solution that mitigates the effect of outliers in the data and helps capture the variations of the language-usage-patterns within a text sample. Specifically, we represent each text sample as a point set and identify the top-k stylistically similar text samples (SSTs) from the corpus. We then apply the probabilistic k nearest neighbors’ classifier on the identified top-k SSTs to predict the native languages of the authors. To conduct experiments, we create three new corpora where each corpus is written in a different language, namely, English, French, and German. Our experimental studies show that our solution outperforms competitive methods and reports more than 80\% accuracy across languages.},
    journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
    month = {apr},
    articleno = {55},
    numpages = {19},
    keywords = {text classification, stylometry, native language identification, Author profiling, forensic investigation}
}

@article{Bangalore,
    author = {Bangalore, Srinivas and Joshi, Aravind K.},
    title = {Supertagging: An Approach to Almost Parsing},
    year = {1999},
    issue_date = {June 1999},
    publisher = {MIT Press},
    address = {Cambridge, MA, USA},
    volume = {25},
    number = {2},
    issn = {0891-2017},
    abstract = {In this paper, we have proposed novel methods for robust parsing that integrate the flexibility of linguistically motivated lexical descriptions with the robustness of statistical techniques. Our thesis is that the computation of linguistic structure can be localized if lexical items are associated with rich descriptions (supertags) that impose complex constraints in a local context. The supertags are designed such that only those elements on which the lexical item imposes constraints appear within a given supertag. Further, each lexical item is associated with as many supertags as the number of different syntactic contexts in which the lexical item can appear. This makes the number of different descriptions for each lexical item much larger than when the descriptions are less complex, thus increasing the local ambiguity for a parser. But this local ambiguity can be resolved by using statistical distributions of supertag co-occurrences collected from a corpus of parses. We have explored these ideas in the context of the Lexicalized Tree-Adjoining Grammar (LTAG) framework. The supertags in LTAG combine both phrase structure information and dependency information in a single representation. Supertag disambiguation results in a representation that is effectively a parse (an almost parse), and the parser need "only" combine the individual supertags. This method of parsing can also be used to parse sentence fragments such as in spoken utterances where the disambiguated supertag sequence may not combine into a single structure.},
    journal = {Comput. Linguist.},
    month = {jun},
    pages = {237–265},
    numpages = {29}
}

@inproceedings{bai,
    title = "Syntax-{BERT}: Improving Pre-trained Transformers with Syntax Trees",
    author = "Bai, Jiangang  and
      Wang, Yujing  and
      Chen, Yiren  and
      Yang, Yaming  and
      Bai, Jing  and
      Yu, Jing  and
      Tong, Yunhai",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.262",
    doi = "10.18653/v1/2021.eacl-main.262",
    pages = "3011--3020",
    abstract = "Pre-trained language models like BERT achieve superior performances in various NLP tasks without explicit consideration of syntactic information. Meanwhile, syntactic information has been proved to be crucial for the success of NLP applications. However, how to incorporate the syntax trees effectively and efficiently into pre-trained Transformers is still unsettled. In this paper, we address this problem by proposing a novel framework named Syntax-BERT. This framework works in a plug-and-play mode and is applicable to an arbitrary pre-trained checkpoint based on Transformer architecture. Experiments on various datasets of natural language understanding verify the effectiveness of syntax trees and achieve consistent improvement over multiple pre-trained models, including BERT, RoBERTa, and T5.",
}

@article{wang-contrastive-learning,
    author       = {Xin Wang and
                  Yasheng Wang and
                  Pingyi Zhou and
                  Fei Mi and
                  Meng Xiao and
                  Yadao Wang and
                  Li Li and
                  Xiao Liu and
                  Hao Wu and
                  Jin Liu and
                  Xin Jiang},
    title        = {{CLSEBERT:} Contrastive Learning for Syntax Enhanced Code Pre-Trained
                  Model},
    journal      = {CoRR},
    volume       = {abs/2108.04556},
    year         = {2021},
    url          = {https://arxiv.org/abs/2108.04556},
    eprinttype    = {arXiv},
    eprint       = {2108.04556},
    timestamp    = {Wed, 13 Sep 2023 15:57:12 +0200},
    biburl       = {https://dblp.org/rec/journals/corr/abs-2108-04556.bib},
    bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{koehn,
    title = "{E}uroparl: A Parallel Corpus for Statistical Machine Translation",
    author = "Koehn, Philipp",
    booktitle = "Proceedings of Machine Translation Summit X: Papers",
    month = sep # " 13-15",
    year = "2005",
    address = "Phuket, Thailand",
    url = "https://aclanthology.org/2005.mtsummit-papers.11",
    pages = "79--86",
    abstract = "We collected a corpus of parallel text in 11 languages from the proceedings of the European Parliament, which are published on the web. This corpus has found widespread use in the NLP community. Here, we focus on its acquisition and its application as training data for statistical machine translation (SMT). We trained SMT systems for 110 language pairs, which reveal interesting clues into the challenges ahead.",
}

@article{lado1957linguistics,
    title={Linguistics across cultures; applied linguistics for language teachers},
    author={Lado, Robert},
    year={1957}
}

@misc{liu2023code,
    title={Code Execution with Pre-trained Language Models}, 
    author={Chenxiao Liu and Shuai Lu and Weizhu Chen and Daxin Jiang and Alexey Svyatkovskiy and Shengyu Fu and Neel Sundaresan and Nan Duan},
    year={2023},
    eprint={2305.05383},
    archivePrefix={arXiv},
    primaryClass={cs.PL},
    url={https://aclanthology.org/2023.findings-acl.308.pdf}
}

@misc{wang2023codet5,
    title={CodeT5+: Open Code Large Language Models for Code Understanding and Generation}, 
    author={Yue Wang and Hung Le and Akhilesh Deepak Gotmare and Nghi D. Q. Bui and Junnan Li and Steven C. H. Hoi},
    year={2023},
    eprint={2305.07922},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2305.07922}
}
@inproceedings{han-etal-2019-joint,
    title = "Joint Event and Temporal Relation Extraction with Shared Representations and Structured Prediction",
    author = "Han, Rujun  and
      Ning, Qiang  and
      Peng, Nanyun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1041",
    doi = "10.18653/v1/D19-1041",
    pages = "434--444",
    abstract = "We propose a joint event and temporal relation extraction model with shared representation learning and structured prediction. The proposed method has two advantages over existing work. First, it improves event representation by allowing the event and relation modules to share the same contextualized embeddings and neural representation learner. Second, it avoids error propagation in the conventional pipeline systems by leveraging structured inference and learning methods to assign both the event labels and the temporal relation labels jointly. Experiments show that the proposed method can improve both event extraction and temporal relation extraction over state-of-the-art systems, with the end-to-end F1 improved by 10{\%} and 6.8{\%} on two benchmark datasets respectively."
}

@misc{transformer,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{svyatkovskiy-generation,
    author       = {Alexey Svyatkovskiy and
                  Shao Kun Deng and
                  Shengyu Fu and
                  Neel Sundaresan},
    title        = {IntelliCode Compose: Code Generation Using Transformer},
    journal      = {CoRR},
    volume       = {abs/2005.08025},
    year         = {2020},
    url          = {https://arxiv.org/abs/2005.08025},
    eprinttype    = {arXiv},
    eprint       = {2005.08025},
    timestamp    = {Fri, 22 May 2020 16:21:28 +0200},
    biburl       = {https://dblp.org/rec/journals/corr/abs-2005-08025.bib},
    bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{BERT,
    author       = {Jacob Devlin and
                  Ming{-}Wei Chang and
                  Kenton Lee and
                  Kristina Toutanova},
    title        = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
                  Understanding},
    journal      = {CoRR},
    volume       = {abs/1810.04805},
    year         = {2018},
    url          = {http://arxiv.org/abs/1810.04805},
    eprinttype    = {arXiv},
    eprint       = {1810.04805},
    timestamp    = {Tue, 30 Oct 2018 20:39:56 +0100},
    biburl       = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
    bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{nositz-pl-dev-env-adopt,
    author = {von Nostitz-Wallwitz, Ivonne and Kr\"{u}ger, Jacob and Leich, Thomas},
    title = {Towards Improving Industrial Adoption: The Choice of Programming Languages and Development Environments},
    year = {2018},
    isbn = {9781450357449},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3195546.3195548},
    doi = {10.1145/3195546.3195548},
    abstract = {While promising software engineering approaches are proposed every day, only few are adapted by professional developers. There are many potential reasons for this, such as, problems in identifying helpful approaches, missing tools, or lacking practical relevance. With our current research, we are concerned to improve the knowledge transfer from research to practice. In this paper, we discuss the impact of development environments and programming languages on knowledge transfer - considering that many scientific approaches and tools are interesting for professional developers, but rarely adopted by them. We base our discussion mainly on our personal experiences with industry-academia collaborations. To determine whether these experiences also apply to other developers, we additionally conducted a survey with 89 participants from academia and industry. The first results of our on-going work indicate a gap between the development environments and programming languages that are supported or used by researchers and those that are applied in industry. Based on our results, we describe initial discussions that can help to improve collaborations between industry and research.},
    booktitle = {Proceedings of the 5th International Workshop on Software Engineering Research and Industrial Practice},
    pages = {10–17},
    numpages = {8},
    location = {Gothenburg, Sweden},
    series = {SER\&IP '18}
}

@inproceedings{meyerovich-empirical-pl-adopt,
    author = {Meyerovich, Leo A. and Rabkin, Ariel S.},
    title = {Empirical Analysis of Programming Language Adoption},
    year = {2013},
    isbn = {9781450323741},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2509136.2509515},
    doi = {10.1145/2509136.2509515},
    abstract = {Some programming languages become widely popular while others fail to grow beyond their niche or disappear altogether. This paper uses survey methodology to identify the factors that lead to language adoption. We analyze large datasets, including over 200,000 SourceForge projects, 590,000 projects tracked by Ohloh, and multiple surveys of 1,000-13,000 programmers.We report several prominent findings. First, language adoption follows a power law; a small number of languages account for most language use, but the programming market supports many languages with niche user bases. Second, intrinsic features have only secondary importance in adoption. Open source libraries, existing code, and experience strongly influence developers when selecting a language for a project. Language features such as performance, reliability, and simple semantics do not. Third, developers will steadily learn and forget languages. The overall number of languages developers are familiar with is independent of age. Finally, when considering intrinsic aspects of languages, developers prioritize expressivity over correctness. They perceive static types as primarily helping with the latter, hence partly explaining the popularity of dynamic languages.},
    booktitle = {Proceedings of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages \& Applications},
    pages = {1–18},
    numpages = {18},
    keywords = {survey research, programming language adoption},
    location = {Indianapolis, Indiana, USA},
    series = {OOPSLA '13}
}

@article{vania-low-resource-parsers,
    author       = {Clara Vania and
                  Yova Kementchedjhieva and
                  Anders S{\o}gaard and
                  Adam Lopez},
    title        = {A systematic comparison of methods for low-resource dependency parsing
                  on genuinely low-resource languages},
    journal      = {CoRR},
    volume       = {abs/1909.02857},
    year         = {2019},
    url          = {http://arxiv.org/abs/1909.02857},
    eprinttype    = {arXiv},
    eprint       = {1909.02857},
    timestamp    = {Mon, 16 Sep 2019 17:27:14 +0200},
    biburl       = {https://dblp.org/rec/journals/corr/abs-1909-02857.bib},
    bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{tarassow2023-lr-code,
    title={The potential of LLMs for coding with low-resource and domain-specific programming languages}, 
    author={Artur Tarassow},
    year={2023},
    eprint={2307.13018},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{chen2022transferability,
    title={On the Transferability of Pre-trained Language Models for Low-Resource Programming Languages}, 
    author={Fuxiang Chen and Fatemeh Fard and David Lo and Timofey Bryksin},
    year={2022},
    eprint={2204.09653},
    archivePrefix={arXiv},
    primaryClass={cs.PL}
}

@inproceedings{ahmed-multilingual,
    author = {Ahmed, Toufique and Devanbu, Premkumar},
    title = {Multilingual Training for Software Engineering},
    year = {2022},
    isbn = {9781450392211},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3510003.3510049},
    doi = {10.1145/3510003.3510049},
    abstract = {Well-trained machine-learning models, which leverage large amounts of open-source software data, have now become an interesting approach to automating many software engineering tasks. Several SE tasks have all been subject to this approach, with performance gradually improving over the past several years with better models and training methods. More, and more diverse, clean, labeled data is better for training; but constructing good-quality datasets is time-consuming and challenging. Ways of augmenting the volume and diversity of clean, labeled data generally have wide applicability. For some languages (e.g., Ruby) labeled data is less abundant; in others (e.g., JavaScript) the available data maybe more focused on some application domains, and thus less diverse. As a way around such data bottlenecks, we present evidence suggesting that human-written code in different languages (which performs the same function), is rather similar, and particularly preserving of identifier naming patterns; we further present evidence suggesting that identifiers are a very important element of training data for software engineering tasks. We leverage this rather fortuitous phenomenon to find evidence that available multilingual training data (across different languages) can be used to amplify performance. We study this for 3 different tasks: code summarization, code retrieval, and function naming. We note that this data-augmenting approach is broadly compatible with different tasks, languages, and machine-learning models.},
    booktitle = {Proceedings of the 44th International Conference on Software Engineering},
    pages = {1443–1455},
    numpages = {13},
    keywords = {method name prediction, code summarization, deep learning, code search},
    location = {Pittsburgh, Pennsylvania},
    series = {ICSE '22}
}

@misc{faisal2022phylogenyinspired,
    title={Phylogeny-Inspired Adaptation of Multilingual Models to New Languages}, 
    author={Fahim Faisal and Antonios Anastasopoulos},
    year={2022},
    eprint={2205.09634},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}


@article{CodeXGlUE,
    author       = {Shuai Lu and
                  Daya Guo and
                  Shuo Ren and
                  Junjie Huang and
                  Alexey Svyatkovskiy and
                  Ambrosio Blanco and
                  Colin B. Clement and
                  Dawn Drain and
                  Daxin Jiang and
                  Duyu Tang and
                  Ge Li and
                  Lidong Zhou and
                  Linjun Shou and
                  Long Zhou and
                  Michele Tufano and
                  Ming Gong and
                  Ming Zhou and
                  Nan Duan and
                  Neel Sundaresan and
                  Shao Kun Deng and
                  Shengyu Fu and
                  Shujie Liu},
    title        = {CodeXGLUE: {A} Machine Learning Benchmark Dataset for Code Understanding
                  and Generation},
    journal      = {CoRR},
    volume       = {abs/2102.04664},
    year         = {2021},
    url          = {https://arxiv.org/abs/2102.04664},
    eprinttype    = {arXiv},
    eprint       = {2102.04664},
    timestamp    = {Wed, 06 Jul 2022 08:37:28 +0200},
    biburl       = {https://dblp.org/rec/journals/corr/abs-2102-04664.bib},
    bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{CodeNet,
    author = {Ruchir Puri and
                  David S. Kung and
                  Geert Janssen and
                  Wei Zhang and
                  Giacomo Domeniconi and
                  Vladimir Zolotov and
                  Julian Dolby and
                  Jie Chen and
                  Mihir R. Choudhury and
                  Lindsey Decker and
                  Veronika Thost and
                  Luca Buratti and
                  Saurabh Pujar and
                  Ulrich Finkler},
    title        = {Project CodeNet: {A} Large-Scale {AI} for Code Dataset for Learning
                  a Diversity of Coding Tasks},
    journal      = {CoRR},
    volume       = {abs/2105.12655},
    year         = {2021},
    url          = {https://arxiv.org/abs/2105.12655},
    eprinttype    = {arXiv},
    eprint       = {2105.12655},
    timestamp    = {Wed, 06 Jul 2022 15:56:31 +0200},
    biburl       = {https://dblp.org/rec/journals/corr/abs-2105-12655.bib},
    bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{TreeBERT,
    author       = {Xue Jiang and
                  Zhuoran Zheng and
                  Chen Lyu and
                  Liang Li and
                  Lei Lyu},
    title        = {TreeBERT: {A} Tree-Based Pre-Trained Model for Programming Language},
    journal      = {CoRR},
    volume       = {abs/2105.12485},
    year         = {2021},
    url          = {https://arxiv.org/abs/2105.12485},
    eprinttype    = {arXiv},
    eprint       = {2105.12485},
    timestamp    = {Tue, 01 Jun 2021 18:07:59 +0200},
    biburl       = {https://dblp.org/rec/journals/corr/abs-2105-12485.bib},
    bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{CuBERT,
    author       = {Aditya Kanade and
                  Petros Maniatis and
                  Gogul Balakrishnan and
                  Kensen Shi},
    title        = {Pre-trained Contextual Embedding of Source Code},
    journal      = {CoRR},
    volume       = {abs/2001.00059},
    year         = {2020},
    url          = {http://arxiv.org/abs/2001.00059},
    eprinttype    = {arXiv},
    eprint       = {2001.00059},
    timestamp    = {Fri, 10 Jan 2020 13:10:19 +0100},
    biburl       = {https://dblp.org/rec/journals/corr/abs-2001-00059.bib},
    bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{CodeExecutor,
      title={Code Execution with Pre-trained Language Models}, 
      author={Chenxiao Liu and Shuai Lu and Weizhu Chen and Daxin Jiang and Alexey Svyatkovskiy and Shengyu Fu and Neel Sundaresan and Nan Duan},
      year={2023},
      eprint={2305.05383},
      archivePrefix={arXiv},
      primaryClass={cs.PL}
}

@misc{CodeBERT,
      title={CodeBERT: A Pre-Trained Model for Programming and Natural Languages}, 
      author={Zhangyin Feng and Daya Guo and Duyu Tang and Nan Duan and Xiaocheng Feng and Ming Gong and Linjun Shou and Bing Qin and Ting Liu and Daxin Jiang and Ming Zhou},
      year={2020},
      eprint={2002.08155},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{llm-survey,
      title={A Survey of Large Language Models}, 
      author={Wayne Xin Zhao and Kun Zhou and Junyi Li and Tianyi Tang and Xiaolei Wang and Yupeng Hou and Yingqian Min and Beichen Zhang and Junjie Zhang and Zican Dong and Yifan Du and Chen Yang and Yushuo Chen and Zhipeng Chen and Jinhao Jiang and Ruiyang Ren and Yifan Li and Xinyu Tang and Zikang Liu and Peiyu Liu and Jian-Yun Nie and Ji-Rong Wen},
      year={2023},
      eprint={2303.18223},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{code-search,
    author = {Gu, Xiaodong and Zhang, Hongyu and Kim, Sunghun},
    title = {Deep Code Search},
    year = {2018},
    isbn = {9781450356381},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3180155.3180167},
    doi = {10.1145/3180155.3180167},
    abstract = {To implement a program functionality, developers can reuse previously written code snippets by searching through a large-scale codebase. Over the years, many code search tools have been proposed to help developers. The existing approaches often treat source code as textual documents and utilize information retrieval models to retrieve relevant code snippets that match a given query. These approaches mainly rely on the textual similarity between source code and natural language query. They lack a deep understanding of the semantics of queries and source code.In this paper, we propose a novel deep neural network named CODEnn (Code-Description Embedding Neural Network). Instead of matching text similarity, CODEnn jointly embeds code snippets and natural language descriptions into a high-dimensional vector space, in such a way that code snippet and its corresponding description have similar vectors. Using the unified vector representation, code snippets related to a natural language query can be retrieved according to their vectors. Semantically related words can also be recognized and irrelevant/noisy keywords in queries can be handled.As a proof-of-concept application, we implement a code search tool named DeepCS using the proposed CODEnn model. We empirically evaluate DeepCS on a large scale codebase collected from GitHub. The experimental results show that our approach can effectively retrieve relevant code snippets and outperforms previous techniques.},
    booktitle = {Proceedings of the 40th International Conference on Software Engineering},
    pages = {933–944},
    numpages = {12},
    keywords = {code search, joint embedding, deep learning},
    location = {Gothenburg, Sweden},
    series = {ICSE '18}
}

@INPROCEEDINGS{clone-detection,
    author={White, Martin and Tufano, Michele and Vendome, Christopher and Poshyvanyk, Denys},
    booktitle={2016 31st IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
    title={Deep learning code fragments for code clone detection}, 
    year={2016},
    volume={},
    number={},
    pages={87-98},
    doi={}
}

@INPROCEEDINGS{defect-prediction,
    author={Li, Jian and He, Pinjia and Zhu, Jieming and Lyu, Michael R.},
    booktitle={2017 IEEE International Conference on Software Quality, Reliability and Security (QRS)}, 
    title={Software Defect Prediction via Convolutional Neural Network}, 
    year={2017},
    volume={},
    number={},
    pages={318-328},
    doi={10.1109/QRS.2017.42}
}

@misc{program-translation,
    title={Tree-to-tree Neural Networks for Program Translation}, 
    author={Xinyun Chen and Chang Liu and Dawn Song},
    year={2018},
    eprint={1802.03691},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}
@misc{SynCoBERT,
    title={SynCoBERT: Syntax-Guided Multi-Modal Contrastive Pre-Training for Code Representation}, 
    author={Xin Wang and Yasheng Wang and Fei Mi and Pingyi Zhou and Yao Wan and Xiao Liu and Li Li and Hao Wu and Jin Liu and Xin Jiang},
    year={2021},
    eprint={2108.04556},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{10.1145/2509136.2509515,
    author = {Meyerovich, Leo A. and Rabkin, Ariel S.},
    title = {Empirical Analysis of Programming Language Adoption},
    year = {2013},
    isbn = {9781450323741},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2509136.2509515},
    doi = {10.1145/2509136.2509515},
    abstract = {Some programming languages become widely popular while others fail to grow beyond their niche or disappear altogether. This paper uses survey methodology to identify the factors that lead to language adoption. We analyze large datasets, including over 200,000 SourceForge projects, 590,000 projects tracked by Ohloh, and multiple surveys of 1,000-13,000 programmers.We report several prominent findings. First, language adoption follows a power law; a small number of languages account for most language use, but the programming market supports many languages with niche user bases. Second, intrinsic features have only secondary importance in adoption. Open source libraries, existing code, and experience strongly influence developers when selecting a language for a project. Language features such as performance, reliability, and simple semantics do not. Third, developers will steadily learn and forget languages. The overall number of languages developers are familiar with is independent of age. Finally, when considering intrinsic aspects of languages, developers prioritize expressivity over correctness. They perceive static types as primarily helping with the latter, hence partly explaining the popularity of dynamic languages.},
    booktitle = {Proceedings of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages \&amp; Applications},
    pages = {1–18},
    numpages = {18},
    keywords = {survey research, programming language adoption},
    location = {Indianapolis, Indiana, USA},
    series = {OOPSLA '13}
}

@article{code-adoption,
    author = {Meyerovich, Leo A. and Rabkin, Ariel S.},
    title = {Empirical Analysis of Programming Language Adoption},
    year = {2013},
    issue_date = {October 2013},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {48},
    number = {10},
    issn = {0362-1340},
    url = {https://doi.org/10.1145/2544173.2509515},
    doi = {10.1145/2544173.2509515},
    abstract = {Some programming languages become widely popular while others fail to grow beyond their niche or disappear altogether. This paper uses survey methodology to identify the factors that lead to language adoption. We analyze large datasets, including over 200,000 SourceForge projects, 590,000 projects tracked by Ohloh, and multiple surveys of 1,000-13,000 programmers.We report several prominent findings. First, language adoption follows a power law; a small number of languages account for most language use, but the programming market supports many languages with niche user bases. Second, intrinsic features have only secondary importance in adoption. Open source libraries, existing code, and experience strongly influence developers when selecting a language for a project. Language features such as performance, reliability, and simple semantics do not. Third, developers will steadily learn and forget languages. The overall number of languages developers are familiar with is independent of age. Finally, when considering intrinsic aspects of languages, developers prioritize expressivity over correctness. They perceive static types as primarily helping with the latter, hence partly explaining the popularity of dynamic languages.},
    journal = {SIGPLAN Not.},
    month = {oct},
    pages = {1–18},
    numpages = {18},
    keywords = {survey research, programming language adoption}
}

@article{py150,
    author = {Raychev, Veselin and Bielik, Pavol and Vechev, Martin},
    title = {Probabilistic Model for Code with Decision Trees},
    year = {2016},
    issue_date = {October 2016},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {51},
    number = {10},
    issn = {0362-1340},
    url = {https://doi.org/10.1145/3022671.2984041},
    doi = {10.1145/3022671.2984041},
    abstract = {In this paper we introduce a new approach for learning precise and general probabilistic models of code based on decision tree learning. Our approach directly benefits an emerging class of statistical programming tools which leverage probabilistic models of code learned over large codebases (e.g., GitHub) to make predictions about new programs (e.g., code completion, repair, etc).  The key idea is to phrase the problem of learning a probabilistic model of code as learning a decision tree in a domain specific language over abstract syntax trees (called TGen). This allows us to condition the prediction of a program element on a dynamically computed context. Further, our problem formulation enables us to easily instantiate known decision tree learning algorithms such as ID3, but also to obtain new variants we refer to as ID3+ and E13, not previously explored and ones that outperform ID3 in prediction accuracy.  Our approach is general and can be used to learn a probabilistic model of any programming language. We implemented our approach in a system called Deep3 and evaluated it for the challenging task of learning probabilistic models of JavaScript and Python. Our experimental results indicate that Deep3 predicts elements of JavaScript and Python code with precision above 82\% and 69\%, respectively. Further, Deep3 often significantly outperforms state-of-the-art approaches in overall prediction accuracy.},
    journal = {SIGPLAN Not.},
    month = {oct},
    pages = {731–747},
    numpages = {17},
    keywords = {Decision Trees, Probabilistic Models of Code, Code Completion}
}

@misc{dedupe,
    title={The Adverse Effects of Code Duplication in Machine Learning Models of Code}, 
    author={Miltiadis Allamanis},
    year={2019},
    eprint={1812.06469},
    archivePrefix={arXiv},
    primaryClass={cs.SE}
}



